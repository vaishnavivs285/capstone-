# Sentiment Analysis on Nykaa Beauty Product Reviews

## ğŸ“Œ Project Overview
This project performs **sentiment analysis on real-world beauty product reviews** collected from the Nykaa e-commerce platform. The goal is to understand customer opinions and compare how different classical machine learning models perform on text data.

The project follows an **end-to-end applied ML workflow**: data collection â†’ cleaning â†’ labeling â†’ feature extraction â†’ model training â†’ evaluation â†’ visualization.

---

## ğŸ“Š Dataset Description
- **Source**: Nykaa product review pages (multiple beauty products)
- **Data type**: User-written textual reviews
- **Final dataset size**: ~100 reviews
- **Sentiment classes**:
  - `strong_positive`
  - `mixed_feedback`

This dataset is intentionally realistic and noisy, unlike clean benchmark datasets.

---

## ğŸ›’ How Data Was Collected
- Nykaa product review pages were accessed manually in a browser.
- Due to anti-scraping restrictions (403 errors), pages were **saved as HTML files locally**.
- Reviews were extracted using **BeautifulSoup** by parsing relevant HTML tags.
- Multiple products were used to ensure **variety in opinions** and reduce bias.

âœ” This approach respects website restrictions while still working with real data.

---

## ğŸ§¹ Data Cleaning Process
The raw scraped data contained:
- Duplicate reviews
- â€œRead Moreâ€ truncation text
- Extra punctuation and casing inconsistencies

Cleaning steps:
- Removed duplicate reviews
- Removed UI text such as `...Read More`
- Converted text to lowercase
- Filtered very short/non-informative reviews

This ensured cleaner and more meaningful text for modeling.

---

## ğŸ·ï¸ How Labeling Was Done
Manual rule-based labeling was applied:
- Reviews with clear praise, strong satisfaction â†’ `strong_positive`
- Reviews with both pros and cons, neutral tone â†’ `mixed_feedback`

Why not automated labeling?
- Dataset is small
- Manual labeling improves **label quality and interpretability**

This mimics real-world scenarios where domain knowledge matters.

---

## ğŸ”¤ Feature Extraction
Two vectorization techniques were used:

### 1ï¸âƒ£ Count Vectorizer
- Converts text into word-frequency vectors
- Simple and effective for probabilistic models
- Used with Naive Bayes

### 2ï¸âƒ£ TF-IDF Vectorizer
- Penalizes common words and boosts informative terms
- Reduces noise from frequently occurring words
- Used with Logistic Regression and Decision Tree

Using both allows comparison of **frequency-based vs importance-based representations**.

---

## ğŸ¤– Models Used (and Why)

### âœ… Naive Bayes
- Works well with sparse, high-dimensional text data
- Fast and probabilistic
- Strong baseline for NLP tasks

### âœ… Logistic Regression
- Linear model with good generalization
- Handles TF-IDF features well
- Often outperforms more complex models on text

### âœ… Decision Tree
- Included for comparison
- Helps demonstrate why tree-based models often struggle with sparse text features

âŒ Deep learning was avoided intentionally due to small dataset size.

---

## ğŸ“ˆ Model Evaluation Strategy
Evaluation was done using:
- **Accuracy**
- **Precision, Recall, F1-score**
- **Confusion Matrix** (for error analysis)

Why confusion matrix?
- Accuracy alone can be misleading
- Shows which sentiment classes are being confused
- Helps identify overfitting (especially in Decision Trees)

---

## âš ï¸ Challenges Faced
- Website scraping restrictions (403 errors)
- Managing Jupyter notebook execution order
- Class imbalance toward positive reviews
- Sparse dataset size limiting model complexity

These challenges reflect **real-world ML issues**, not textbook problems.

---

## ğŸš€ Possible Future Improvements
- Add neutral/negative sentiment by including low-rated products
- Use cross-validation instead of single split
- Experiment with bigrams/trigrams tuning
- Try word embeddings (Word2Vec, GloVe)
- Deploy as a simple web app for live predictions

---

## ğŸ¯ Practical Use of This Project
- Understand customer sentiment trends
- Help brands analyze product feedback
- Identify mixed or borderline reviews
- Serve as a template for real-world NLP pipelines

---

## ğŸ§  Key Learnings
- Classical ML models are still very effective for NLP
- Feature engineering matters more than model complexity
- Proper evaluation is critical for trustworthy results

---

## ğŸ“ Tech Stack
- Python
- Pandas, NumPy
- BeautifulSoup
- Scikit-learn
- Matplotlib

---

## ğŸ‘¤ Author
**Praveen Kumar**

---

â­ If you found this project useful, feel free to fork or star the repository!

